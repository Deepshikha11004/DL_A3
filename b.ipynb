{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available for computation.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training samples: \", len(trainset))\n",
    "\n",
    "\n",
    "print(trainloader.dataset.data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LayerNormMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mlp_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        print(\"mlp_output (LayerNorm): \", mlp_output.size())\n",
    "        residual = mlp_output + x\n",
    "        print(\"residual(LayerNorm): \", residual.size())\n",
    "        normalized_output = self.layer_norm(residual)\n",
    "        print(\"normalized_output(LayerNorm): \", normalized_output.size())\n",
    "        return normalized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAttention(nn.Module):\n",
    "    def __init__(self, input_dim,heads):\n",
    "        super(SAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, heads)\n",
    "        self.key = nn.Linear(input_dim, heads)\n",
    "        self.value = nn.Linear(input_dim, heads)\n",
    "        # self.W_o = nn.Linear(input_dim, heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # batch_size , num_patches , input_dim = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        att = self.scaled_dot_product_attention(q, k, v)\n",
    "        print(\"att:\",att.size())\n",
    "        # output = self.W_o(att)\n",
    "        # print(\"output:\",output.size())\n",
    "        return att\n",
    "    \n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        d_k = q.size(-1)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        print(\"scores:\",scores.size())\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        print(\"attention:\",attention.size())\n",
    "        \n",
    "        energy = torch.matmul(attention, v)\n",
    "        print(\"energy:\",energy.size())\n",
    "        return energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.head_dim = input_dim // heads\n",
    "        self.attention_heads = nn.ModuleList([SAttention(input_dim = input_dim , heads = self.head_dim ) for _ in range(heads)])\n",
    "        self.W_o = nn.Linear(input*heads, input)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention = [attention_head(x) for attention_head in self.attention_heads]\n",
    "        attention = torch.cat(attention, dim=-1)\n",
    "        print(\"attention cat:\",attention.size())\n",
    "        output = self.W_o(attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.linear1(x))\n",
    "        mlp_output = self.linear2(x)\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads,mlp_ratio):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(input)\n",
    "        self.attention = MultiHeadAttention(input_dim, num_heads)\n",
    "        self.layer_norm2 = nn.LayerNorm(input)\n",
    "        hidden_feat = int(input_dim * mlp_ratio)\n",
    "        self.mlp = MLP(input_dim, hidden_feat, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norm1(x))\n",
    "        x = x + self.mlp(self.layer_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = nn.Conv2d(3, 64, kernel_size=16, stride=16)\n",
    "        self.class_token = nn.Parameter(torch.randn(1, self.patch_embed.out_channels, 1))\n",
    "        self.transformer = nn.ModuleList([TransformerLayer(input_dim = 64, num_heads = 8, mlp_ratio = 4) for _ in range(layers)])\n",
    "        self.pos_enc = nn.Parameter(torch.randn(1, self.patch_embed.out_channels, 1))\n",
    "        self.layer_norm = nn.LayerNorm(64)\n",
    "        self.linear = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.class_token\n",
    "        x = x + self.pos_enc\n",
    "        x = self.layer_norm(x)\n",
    "        for transformer in self.transformer:\n",
    "            x = transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(layers = 12)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs = data[0]\n",
    "        labels = data[1]\n",
    "        inputs.to(device)\n",
    "        labels.to(device)\n",
    "        print(\"inputs: \", inputs.size())\n",
    "        print(\"labels: \", labels.size())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {running_loss/100}\")\n",
    "            running_loss = 0.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
